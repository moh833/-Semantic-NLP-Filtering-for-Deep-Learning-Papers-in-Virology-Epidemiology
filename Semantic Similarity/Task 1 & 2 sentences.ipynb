{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "133a2350-c8df-43a1-803c-f8394e96bc36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b93816d0-e4fb-4874-be33-8952690c241f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/collection_with_abstracts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "251dde0f-00d6-40b8-a6b1-318adc86d4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Abstract'] = df['Abstract'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c7700ef-3d57-45a6-9599-e3bceb57ed59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_data'] = df.apply(lambda row: row['Title'] + '\\n\\n' + row['Abstract'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42ceef6-6189-4dfb-8bcf-a9daef70debc",
   "metadata": {},
   "source": [
    "# Semantic Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d501f3-4351-4101-b43a-a3f084625658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_mining_phrases = [\"natural language processing\", \"text mining\", \"NLP\", \"computational linguistics\", \"language processing\", \"text analytics\", \"textual data analysis\", \"text data analysis\", \"text analysis\", \"speech and language technology\", \"language modeling\", \"computational semantics\"]\n",
    "# computer_vision_phrases = [\"computer vision\", \"vision model\", \"image processing\", \"vision algorithms\", \"computer graphics and vision\", \"object recognition\", \"scene understanding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a99be6a-40cb-4a4c-8f75-fff0370814dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Mas2\\Work\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import spacy\n",
    "\n",
    "# sbert_model = SentenceTransformer('all-MiniLM-L6-v2')  # Use any preferred SBERT model\n",
    "bi_encoder = SentenceTransformer('msmarco-distilbert-base-v4')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def split_paragraph_to_sentences(paragraph):\n",
    "    # Split the paragraph into sentences using spaCy\n",
    "    doc = nlp(paragraph)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents]\n",
    "    return sentences\n",
    "\n",
    "def encode_paragraphs(paragraphs):\n",
    "\n",
    "    paragraph_sentence_embeddings = []\n",
    "\n",
    "    for PMID, paragraph in paragraphs:\n",
    "        sentences = split_paragraph_to_sentences(paragraph)\n",
    "\n",
    "        sentence_embeddings = bi_encoder.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "        paragraph_sentence_embeddings.append({\n",
    "            \"PMID\": PMID,\n",
    "            \"paragraph\": paragraph,\n",
    "            \"sentences\": sentences,\n",
    "            \"sentence_embeddings\": sentence_embeddings\n",
    "        })\n",
    "\n",
    "    return paragraph_sentence_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87b0d343-b4a6-4dee-9e27-18e58e957955",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = df[['PMID', 'text_data']].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8348f8af-5acd-4d40-94fd-2e3856d03510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[39435445,\n",
       " 'Editorial: The operationalization of cognitive systems in the comprehension of visual structures\\n\\n']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1109f4e4-42a6-4b05-a146-8ef7e59089f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode paragraphs\n",
    "paragraph_sentence_embeddings = encode_paragraphs(paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d290ab-f12a-494f-82b5-37e122c90122",
   "metadata": {},
   "source": [
    "# Save and Load paragraph_sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cc660cf9-a92c-435f-8050-20ab6a5a2aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open(\"embeddings/paragraph_sentence_embeddings.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(paragraph_sentence_embeddings, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf2b0124-e18a-4d46-9c71-ed368059781e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"embeddings/paragraph_sentence_embeddings.pkl\", \"rb\") as file:\n",
    "    paragraph_sentence_embeddings = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a62dacd4-5854-48ff-b412-f1ad4d47ec07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PMID': 39398866,\n",
       " 'paragraph': \"Characterization of arteriosclerosis based on computer-aided measurements of intra-arterial thickness\\n\\nPURPOSE: Our purpose is to develop a computer vision approach to quantify intra-arterial thickness on digital pathology images of kidney biopsies as a computational biomarker of arteriosclerosis.\\r\\nAPPROACH: The severity of the arteriosclerosis was scored (0 to 3) in 753 arteries from 33 trichrome-stained whole slide images (WSIs) of kidney biopsies, and the outer contours of the media, intima, and lumen were manually delineated by a renal pathologist. We then developed a multi-class deep learning (DL) framework for segmenting the different intra-arterial compartments (training dataset: 648 arteries from 24 WSIs; testing dataset: 105 arteries from 9 WSIs). Subsequently, we employed radial sampling and made measurements of media and intima thickness as a function of spatially encoded polar coordinates throughout the artery. Pathomic features were extracted from the measurements to collectively describe the arterial wall characteristics. The technique was first validated through numerical analysis of simulated arteries, with systematic deformations applied to study their effect on arterial thickness measurements. We then compared these computationally derived measurements with the pathologists' grading of arteriosclerosis.\\r\\nRESULTS: Numerical validation shows that our measurement technique adeptly captured the decreasing smoothness in the intima and media thickness as the deformation increases in the simulated arteries. Intra-arterial DL segmentations of media, intima, and lumen achieved Dice scores of 0.84, 0.78, and 0.86, respectively. Several significant associations were identified between arteriosclerosis grade and pathomic features using our technique (e.g., intima-media ratio average [ τ = 0.52  , p &lt; 0.0001  ]) through Kendall's tau analysis.\\r\\nCONCLUSIONS: We developed a computer vision approach to computationally characterize intra-arterial morphology on digital pathology images and demonstrate its feasibility as a potential computational biomarker of arteriosclerosis.\",\n",
       " 'sentences': ['Characterization of arteriosclerosis based on computer-aided measurements of intra-arterial thickness\\n\\nPURPOSE: Our purpose is to develop a computer vision approach to quantify intra-arterial thickness on digital pathology images of kidney biopsies as a computational biomarker of arteriosclerosis.',\n",
       "  'APPROACH: The severity of the arteriosclerosis was scored (0 to 3) in 753 arteries from 33 trichrome-stained whole slide images (WSIs) of kidney biopsies, and the outer contours of the media, intima, and lumen were manually delineated by a renal pathologist.',\n",
       "  'We then developed a multi-class deep learning (DL) framework for segmenting the different intra-arterial compartments (training dataset: 648 arteries from 24 WSIs; testing dataset: 105 arteries from 9 WSIs).',\n",
       "  'Subsequently, we employed radial sampling and made measurements of media and intima thickness as a function of spatially encoded polar coordinates throughout the artery.',\n",
       "  'Pathomic features were extracted from the measurements to collectively describe the arterial wall characteristics.',\n",
       "  'The technique was first validated through numerical analysis of simulated arteries, with systematic deformations applied to study their effect on arterial thickness measurements.',\n",
       "  \"We then compared these computationally derived measurements with the pathologists' grading of arteriosclerosis.\\r\\nRESULTS: Numerical validation shows that our measurement technique adeptly captured the decreasing smoothness in the intima and media thickness as the deformation increases in the simulated arteries.\",\n",
       "  'Intra-arterial DL segmentations of media, intima, and lumen achieved Dice scores of 0.84, 0.78, and 0.86, respectively.',\n",
       "  \"Several significant associations were identified between arteriosclerosis grade and pathomic features using our technique (e.g., intima-media ratio average [ τ = 0.52  , p &lt; 0.0001  ]) through Kendall's tau analysis.\",\n",
       "  'CONCLUSIONS: We developed a computer vision approach to computationally characterize intra-arterial morphology on digital pathology images and demonstrate its feasibility as a potential computational biomarker of arteriosclerosis.'],\n",
       " 'sentence_embeddings': tensor([[-0.4212,  0.1281,  0.2384,  ...,  0.3343, -0.0787,  0.8516],\n",
       "         [ 0.2129, -0.4385,  0.3467,  ...,  0.4797,  0.5661,  0.3115],\n",
       "         [ 0.2115, -1.3426, -0.3432,  ...,  0.1430, -0.5962,  0.2395],\n",
       "         ...,\n",
       "         [ 0.0555, -0.9332,  0.4594,  ...,  0.2181, -0.1801, -0.0037],\n",
       "         [-0.4590, -0.1414,  0.0497,  ...,  0.2221,  0.3314,  0.1166],\n",
       "         [-0.9022,  0.0445, -0.2354,  ...,  0.0739,  0.0696,  0.8237]],\n",
       "        device='cuda:0')}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph_sentence_embeddings[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1747f132-e631-4f5d-9bb8-f8be63485bb2",
   "metadata": {},
   "source": [
    "# Filter out irrelevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d28ffb98-26c4-44e2-ad38-bf271af50a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import util\n",
    "\n",
    "def get_paragraph_similarity_with_query(query, paragraph_sentence_embeddings):\n",
    "    query_embedding = bi_encoder.encode(query, convert_to_tensor=True)\n",
    "\n",
    "    paragraph_scores = []\n",
    "\n",
    "    for paragraph_data in paragraph_sentence_embeddings:\n",
    "\n",
    "        sentence_embeddings = paragraph_data[\"sentence_embeddings\"]\n",
    "\n",
    "        similarities = util.cos_sim(query_embedding, sentence_embeddings)[0]\n",
    "\n",
    "        max_similarity = similarities.max().item()\n",
    "\n",
    "        max_sim_ind = similarities.argmax().item()\n",
    "\n",
    "        # paragraph_scores.append((paragraph_data[\"paragraph\"], max_similarity))\n",
    "        # paragraph_scores.append((paragraph_data[\"sentences\"][max_sim_ind], max_similarity))\n",
    "        paragraph_scores.append({'PMID': paragraph_data['PMID'], 'paragraph': paragraph_data[\"paragraph\"], 'max_sentence': paragraph_data[\"sentences\"][max_sim_ind], 'max_sentence_score': max_similarity})\n",
    "\n",
    "    return paragraph_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15e76e9d-c6e3-4cda-a93f-fbc7d7e29aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matches(query, threshold):\n",
    "    paragraph_scores = get_paragraph_similarity_with_query(query, paragraph_sentence_embeddings)\n",
    "    paragraph_scores.sort(key=lambda x: x['max_sentence_score'], reverse=True)\n",
    "    # paragraph_scores_filtered = [p for p in paragraph_scores if p['max_sentence_score'] >= threshold]\n",
    "    return set([p['PMID'] for p in paragraph_scores if p['max_sentence_score'] >= threshold])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27d18a03-0fc1-4071-bfef-93b10ac73bad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined = (get_matches(\"virology\", 0.15) | get_matches(\"epidemiology\", 0.15)) & get_matches(\"deep learning\", 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e31984d-85df-4091-98ac-afc28cd31b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4959"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "603d9d00-4ec9-40e5-88d2-545aaac883a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_sentence_embeddings_filtered = [ p for p in paragraph_sentence_embeddings if p['PMID'] in combined ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55d7a79b-dbfd-4c30-ac4f-9449be8dda45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4959"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paragraph_sentence_embeddings_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdba329-b426-4931-a640-7fd02ad06782",
   "metadata": {},
   "source": [
    "# Classify papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b053d3e-453f-42d0-89e2-18a5d6f44740",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_mining_phrases = [\"natural language processing\", \"text mining\", \"NLP\", \"computational linguistics\", \"language processing\", \"text analytics\", \"textual data analysis\", \"text data analysis\", \"text analysis\", \"speech and language technology\", \"language modeling\", \"computational semantics\"]\n",
    "computer_vision_phrases = [\"computer vision\", \"vision model\", \"image processing\", \"vision algorithms\", \"computer graphics and vision\", \"object recognition\", \"scene understanding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f50a5d2-5b7a-49c7-8493-57d22c3234f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import util\n",
    "import numpy as np\n",
    "\n",
    "def get_similarity_with_cv_text_queries(computer_vision_phrases, text_mining_phrases, paragraph_sentence_embeddings):\n",
    "    \n",
    "    computer_vision_embeddings = bi_encoder.encode(computer_vision_phrases, convert_to_tensor=True)\n",
    "    text_mining_embeddings = bi_encoder.encode(text_mining_phrases, convert_to_tensor=True)\n",
    "\n",
    "    paragraph_scores = []\n",
    "\n",
    "    # Loop through each paragraph's precomputed sentence embeddings\n",
    "    for paragraph_data in paragraph_sentence_embeddings:\n",
    "\n",
    "\n",
    "        sentence_embeddings = paragraph_data[\"sentence_embeddings\"]\n",
    "        \n",
    "        cv_similarities = util.cos_sim(computer_vision_embeddings, sentence_embeddings)\n",
    "        text_similarities = util.cos_sim(text_mining_embeddings, sentence_embeddings)\n",
    "        \n",
    "        cv_max_score = util.cos_sim(computer_vision_embeddings, sentence_embeddings).max().item()\n",
    "        text_max_score = util.cos_sim(text_mining_embeddings, sentence_embeddings).max().item()\n",
    "        \n",
    "        \n",
    "        max_index_1 = np.unravel_index(np.argmax(cv_similarities.cpu().numpy()), cv_similarities.shape)\n",
    "        max_index_2 = np.unravel_index(np.argmax(text_similarities.cpu().numpy()), text_similarities.shape)\n",
    "        \n",
    "        computer_vision_max_sentence = paragraph_data['sentences'][max_index_1[1]]\n",
    "        computer_vision_max_phrase = computer_vision_phrases[max_index_1[0]]\n",
    "        \n",
    "        text_mining_max_sentence = paragraph_data['sentences'][max_index_2[1]]\n",
    "        text_mining_max_phrase = text_mining_phrases[max_index_2[0]]\n",
    "        \n",
    "        data = {'PMID': paragraph_data['PMID'], 'computer_vision_max_phrase': computer_vision_max_phrase, 'computer_vision_max_sentence': computer_vision_max_sentence, 'computer_vision_max_score': cv_max_score, 'text_mining_max_phrase': text_mining_max_phrase,'text_mining_max_sentence': text_mining_max_sentence, 'text_mining_max_score': text_max_score}\n",
    "        paragraph_scores.append(data)\n",
    "        \n",
    "    return paragraph_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10f5e39e-84e3-47d1-93d8-890d874c20e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_scores_cv_text = get_similarity_with_cv_text_queries(computer_vision_phrases, text_mining_phrases, paragraph_sentence_embeddings_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "510f4087-75a3-4357-b071-9196ee68b229",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4959"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paragraph_scores_cv_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d53b3536-4103-4a57-817d-16469ea28abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_paragraphs(paragraph_scores_cv_text, threshold=0.3):\n",
    "    for data in paragraph_scores_cv_text:\n",
    "        label = ''\n",
    "        if data['computer_vision_max_score'] >= threshold and data['text_mining_max_score'] >= threshold:\n",
    "            label = 'both'\n",
    "        elif data['computer_vision_max_score'] >= threshold:\n",
    "            label = 'computer vision'\n",
    "        elif data['text_mining_max_score'] >= threshold:\n",
    "            label = 'text mining'\n",
    "        else:\n",
    "            label = 'other'\n",
    "        data['label'] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "220d2288-456b-4839-b0ff-f20fa00d9233",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_paragraphs(paragraph_scores_cv_text, threshold=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f425075d-f576-4ac5-984e-ba66eb02e62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [ d['label'] for d in paragraph_scores_cv_text ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a4b7a65a-2e07-40f6-b811-98500cea3420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'other': 1554,\n",
       "         'text mining': 1333,\n",
       "         'both': 1211,\n",
       "         'computer vision': 861})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafce336-41a4-4cf6-aea0-eed04c0a5369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0ef9e8-7187-4710-a64a-5353b6c67857",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
